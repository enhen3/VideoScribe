#!/usr/bin/env python3
"""å…±äº«å·¥å…·ï¼šå¹³å°è¯†åˆ«ã€å…ƒä¿¡æ¯æŠ“å–ã€å­—å¹•/éŸ³é¢‘å¤„ç†ã€Markdown ç”Ÿæˆç­‰ã€‚"""
from __future__ import annotations

import json
import os
import re
import shutil
import tempfile
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from threading import Lock
from typing import Callable, Dict, Iterable, List, Optional, Tuple
from urllib.parse import parse_qs, urlparse

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    try:  # pragma: no cover
        from urllib3.exceptions import NotOpenSSLWarning
    except Exception:
        NotOpenSSLWarning = None  # type: ignore

if NotOpenSSLWarning:
    warnings.filterwarnings("ignore", category=NotOpenSSLWarning)

import requests
import yaml

try:
    from yt_dlp import YoutubeDL
except ImportError:  # pragma: no cover
    YoutubeDL = None  # type: ignore

try:
    from opencc import OpenCC
except ImportError:  # pragma: no cover
    OpenCC = None  # type: ignore

try:
    import whisper
except ImportError:  # pragma: no cover
    whisper = None  # type: ignore


USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
)
BILIBILI_VIEW_API = "https://api.bilibili.com/x/web-interface/view"
BILIBILI_SUBTITLE_API = "https://api.bilibili.com/x/player/v2"
PREFERRED_LANGS = ["zh-Hans", "zh", "zh-Hant", "yue"]
ENGLISH_LANGS = ["en", "en-us", "en-gb"]

# å¹¶å‘å¤„ç†é…ç½®
DEFAULT_MAX_WORKERS = 5  # é»˜è®¤å¹¶å‘æ•°ï¼ˆä¼˜åŒ–ï¼šæé«˜å¹¶å‘ä»¥åŠ å¿«å­—å¹•ä¸‹è½½ï¼‰
MAX_WORKERS_LIMIT = 8  # æœ€å¤§å¹¶å‘æ•°é™åˆ¶
ENABLE_CONCURRENT = True  # é»˜è®¤å¯ç”¨å¹¶å‘
FAV_LIST_RE = re.compile(r"/list/ml(\d+)")
FAV_QUERY_KEYS = ("fid", "media_id", "mlid")
SERIES_LIST_RE = re.compile(r"/list/series/(\d+)")
LANGUAGE_AUTO = "auto"
LANGUAGE_ZH = "zh"
LANGUAGE_EN = "en"
LANGUAGE_CHOICES = {LANGUAGE_AUTO, LANGUAGE_ZH, LANGUAGE_EN}
def _resolve_output_root() -> Path:
    env_path = os.getenv("TRANSCRIBE_OUTPUT_DIR") or os.getenv("BILI_OUTPUT_DIR")
    if env_path:
        path = Path(env_path).expanduser()
        if not path.is_absolute():
            path = Path.home() / path
        return path
    return Path.home() / "ViedoTextDownload"


DEFAULT_OUTPUT_ROOT = _resolve_output_root()
EXTRA_BIN_DIRS = (
    "/opt/homebrew/bin",
    "/usr/local/bin",
    str(Path.home() / "Library/Python/3.9/bin"),
    str(Path.home() / ".local/bin"),
)
DEFAULT_LANGUAGE = "Chinese"
TIMECODE_RE = re.compile(
    r"(?P<start>\d{1,2}:\d{2}:\d{2}(?:[.,]\d{3})?)\s*-->\s*(?P<end>\d{1,2}:\d{2}:\d{2}(?:[.,]\d{3})?)"
)
TAG_RE = re.compile(r"<[^>]+>")


class VideoProcessingError(Exception):
    """ç»Ÿä¸€å¤„ç†å¼‚å¸¸ã€‚"""


@dataclass
class Segment:
    start: float
    end: float
    text: str


@dataclass
class VideoMeta:
    platform: str
    video_id: str
    title: str
    uploader: str
    upload_date: str
    source: str
    url: str
    duration: str
    processed_at: str
    language: str = DEFAULT_LANGUAGE
    original_language: str = "unknown"
    tags: List[str] = field(default_factory=list)


@dataclass
class ProcessResult:
    platform: str
    markdown_path: Path
    txt_path: Optional[Path]
    meta: VideoMeta


if OpenCC:
    _CC = OpenCC("t2s")
else:
    _CC = None


def ensure_extra_path() -> None:
    path_parts = os.environ.get("PATH", "").split(os.pathsep) if os.environ.get("PATH") else []
    changed = False
    for directory in EXTRA_BIN_DIRS:
        if directory and directory not in path_parts:
            path_parts.append(directory)
            changed = True
    if changed:
        os.environ["PATH"] = os.pathsep.join(part for part in path_parts if part)


def ensure_ffmpeg_available() -> None:
    ensure_extra_path()
    if shutil.which("ffmpeg"):
        return
    raise VideoProcessingError(
        "æœªæ£€æµ‹åˆ° ffmpegï¼Œå¯é€šè¿‡ `brew install ffmpeg` å®‰è£…ï¼Œæˆ–è®¾ç½® PATH / TRANSCRIBE_OUTPUT_DIRã€‚"
    )


def _maybe_log(logger: Optional[Callable[[str], None]], message: str) -> None:
    if logger:
        logger(message)


def detect_platform(text: str) -> Optional[str]:
    """æ ¹æ®è¾“å…¥åˆ¤æ–­å¹³å°ã€‚"""
    lowered = text.lower()
    if "bilibili.com" in lowered or "b23.tv" in lowered or re.search(r"bv[0-9a-z]+", lowered, re.I):
        return "bilibili"
    if "youtube.com" in lowered or "youtu.be" in lowered:
        return "youtube"
    return None


def slugify(value: str, fallback: str = "video") -> str:
    value = re.sub(r"[\\/:*?\"<>|]+", "_", value)
    value = re.sub(r"\s+", "_", value)
    value = value.strip("._")
    return value or fallback


def format_timestamp(seconds: Optional[float]) -> str:
    if seconds is None:
        return "unknown"
    total = max(0, int(round(seconds)))
    hours, remainder = divmod(total, 3600)
    minutes, secs = divmod(remainder, 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"


def to_simplified(text: str) -> str:
    if not text:
        return ""
    if _CC is None:
        return text
    try:
        return _CC.convert(text)
    except Exception:  # pragma: no cover
        return text


def normalize_text(text: str, convert_to_simplified: bool = True) -> str:
    cleaned = re.sub(r"\s+", " ", text).strip()
    if not cleaned:
        return ""
    return to_simplified(cleaned) if convert_to_simplified else cleaned


def is_chinese_lang(value: Optional[str]) -> bool:
    if not value:
        return False
    lower = value.lower()
    return lower.startswith(("zh", "chinese", "yue"))


def is_english_language(value: Optional[str]) -> bool:
    if not value:
        return False
    lower = value.lower()
    return lower.startswith("en") or "english" in lower


def looks_like_english(text: Optional[str]) -> bool:
    if not text:
        return False
    ascii_letters = sum(ch.isalpha() and ch.isascii() for ch in text)
    cjk_letters = sum(_contains_cjk_char(ch) for ch in text)
    if ascii_letters == 0:
        return False
    total_letters = ascii_letters + cjk_letters
    if total_letters == 0:
        total_letters = len(text)
    ratio = ascii_letters / max(total_letters, 1)
    return ratio >= 0.6 and cjk_letters < ascii_letters / 2


def _contains_cjk_char(ch: str) -> bool:
    return "\u4e00" <= ch <= "\u9fff"


def contains_chinese(text: Optional[str]) -> bool:
    if not text:
        return False
    return any(_contains_cjk_char(ch) for ch in text)


def normalize_language_mode(mode: Optional[str]) -> str:
    if not mode:
        return LANGUAGE_AUTO
    normalized = mode.strip().lower()
    if normalized not in LANGUAGE_CHOICES:
        return LANGUAGE_AUTO
    return normalized


def should_prefer_english(language_mode: str, texts: Iterable[Optional[str]], fallback: bool = False, audio_language: Optional[str] = None) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¼˜å…ˆä½¿ç”¨è‹±æ–‡ã€‚

    ä¼˜å…ˆçº§ï¼š
    1. ç”¨æˆ·æ˜ç¡®æŒ‡å®šè¯­è¨€ï¼ˆ--lang en/zhï¼‰
    2. éŸ³é¢‘è¯­è¨€ä¿¡æ¯ï¼ˆaudio_language å‚æ•°ï¼‰
    3. æ–‡æœ¬å†…å®¹åˆ†æï¼ˆæ ‡é¢˜ã€æè¿°ç­‰ï¼‰
    4. fallback é»˜è®¤å€¼
    """
    normalized = normalize_language_mode(language_mode)
    if normalized == LANGUAGE_EN:
        return True
    if normalized == LANGUAGE_ZH:
        return False

    # å¦‚æœæœ‰éŸ³é¢‘è¯­è¨€ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨
    if audio_language:
        if is_english_language(audio_language):
            return True
        if is_chinese_lang(audio_language):
            return False

    # åˆ†ææ–‡æœ¬å†…å®¹
    for text in texts:
        if looks_like_english(text):
            return True
    return fallback


def ensure_output_dir(platform: str, uploader: str, root: Path = DEFAULT_OUTPUT_ROOT) -> Path:
    uploader_slug = slugify(uploader or "unknown_creator", "unknown_creator")
    path = root / platform / uploader_slug
    path.mkdir(parents=True, exist_ok=True)
    return path


def segments_to_plain_text(segments: List[Segment]) -> str:
    return "\n".join(seg.text for seg in segments if seg.text).strip()


def write_legacy_txt(segments: List[Segment], output_dir: Path, base_name: str) -> Path:
    txt_path = output_dir / f"{base_name}.txt"
    content = segments_to_plain_text(segments)
    if content:
        txt_path.write_text(content + "\n", encoding="utf-8")
    else:
        txt_path.write_text("", encoding="utf-8")
    return txt_path


def generate_markdown(meta: VideoMeta, segments: List[Segment], output_dir: Path, skip_existing: bool = True) -> Path:
    """æ ¹æ®æ¨¡æ¿ç”Ÿæˆ Markdown æ–‡ä»¶ã€‚"""
    filename = f"{meta.video_id}_{slugify(meta.title)}.md"
    output_dir.mkdir(parents=True, exist_ok=True)
    md_path = output_dir / filename

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if skip_existing and md_path.exists():
        return md_path
    metadata = {
        "platform": meta.platform,
        "video_id": meta.video_id,
        "title": meta.title,
        "uploader": meta.uploader,
        "upload_date": meta.upload_date,
        "source": meta.source,
        "language": meta.language,
        "original_language": meta.original_language,
        "duration": meta.duration,
        "tags": meta.tags or [],
        "processed_at": meta.processed_at,
    }
    front_matter = yaml.safe_dump(metadata, sort_keys=False, allow_unicode=True).strip()
    lines: List[str] = [
        "---",
        front_matter,
        "---",
        "",
        f"# {meta.title}",
        "",
        "## å…ƒä¿¡æ¯ï¼ˆMetadataï¼‰",
        "",
        f"- å¹³å°ï¼š{meta.platform}",
        f"- è§†é¢‘é“¾æ¥ï¼š{meta.url}",
        f"- è§†é¢‘ IDï¼š{meta.video_id}",
        f"- ä¸Šä¼ è€…ï¼š{meta.uploader}",
        f"- ä¸Šä¼ æ—¶é—´ï¼š{meta.upload_date}",
        f"- å­—å¹•æ¥æºï¼š{meta.source}",
        f"- å¤„ç†æ—¶é—´ï¼š{meta.processed_at}",
        f"- è§†é¢‘æ—¶é•¿ï¼š{meta.duration}",
        "",
        "---",
        "",
        "## è§†é¢‘æ‘˜è¦ï¼ˆå¯ç•™ç©ºï¼‰",
        "ï¼ˆä¾›æœªæ¥ AI è‡ªåŠ¨æ€»ç»“ï¼‰",
        "",
        "---",
        "",
        "## æ–‡æœ¬æ­£æ–‡ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰",
        "",
    ]

    for segment in sorted(segments, key=lambda seg: seg.start):
        start_ts = format_timestamp(segment.start)
        end_ts = format_timestamp(segment.end)
        text = segment.text.strip()
        if not text:
            continue
        lines.append(f"### [{start_ts} â†’ {end_ts}]")
        lines.append(text)
        lines.append("")

    final_md_path = output_dir / filename
    final_md_path.write_text("\n".join(lines).strip() + "\n", encoding="utf-8")
    return final_md_path


def parse_timestamp_to_seconds(value: str) -> float:
    value = value.replace(",", ".")
    parts = value.split(":")
    parts = [float(part) for part in parts]
    while len(parts) < 3:
        parts.insert(0, 0.0)
    hours, minutes, seconds = parts
    return hours * 3600 + minutes * 60 + seconds


def parse_subtitle_text(text: str) -> List[Segment]:
    segments: List[Segment] = []
    start: Optional[float] = None
    end: Optional[float] = None
    buffer: List[str] = []

    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line or line.upper().startswith(("WEBVTT", "NOTE", "STYLE", "REGION")):
            if start is not None and buffer:
                combined = TAG_RE.sub("", " ".join(buffer)).strip()
                if combined:
                    segments.append(Segment(start=start, end=end or (start + 1), text=combined))
            start = end = None
            buffer = []
            continue

        match = TIMECODE_RE.match(line)
        if match:
            if start is not None and buffer:
                combined = TAG_RE.sub("", " ".join(buffer)).strip()
                if combined:
                    segments.append(Segment(start=start, end=end or (start + 1), text=combined))
            start = parse_timestamp_to_seconds(match.group("start"))
            end = parse_timestamp_to_seconds(match.group("end"))
            buffer = []
            continue

        if line.startswith("NOTE"):
            continue
        if line.isdigit():
            continue
        buffer.append(line)

    if start is not None and buffer:
        combined = TAG_RE.sub("", " ".join(buffer)).strip()
        if combined:
            segments.append(Segment(start=start, end=end or (start + 1), text=combined))

    return segments


def download_text(url: str) -> str:
    try:
        resp = requests.get(url, timeout=15, headers={"User-Agent": USER_AGENT})
        resp.raise_for_status()
    except requests.RequestException as exc:
        raise VideoProcessingError(f"å­—å¹•ä¸‹è½½å¤±è´¥ï¼š{exc}") from exc
    resp.encoding = resp.encoding or "utf-8"
    return resp.text


def _require_ytdlp() -> None:
    if YoutubeDL is None:
        raise VideoProcessingError("æœªæ£€æµ‹åˆ° yt-dlpï¼Œè¯·å…ˆè¿è¡Œå®‰è£…è„šæœ¬ã€‚")


def _require_whisper() -> None:
    if whisper is None:
        raise VideoProcessingError("æœªå®‰è£… openai-whisperï¼Œè¯·å…ˆè¿è¡Œå®‰è£…è„šæœ¬ã€‚")


def _fmt_upload_date_from_epoch(epoch: Optional[int]) -> str:
    if not epoch:
        return "unknown"
    try:
        return datetime.utcfromtimestamp(epoch).strftime("%Y-%m-%d")
    except Exception:  # pragma: no cover
        return "unknown"


def _fmt_upload_date_from_str(date_str: Optional[str]) -> str:
    if not date_str:
        return "unknown"
    if re.fullmatch(r"\d{8}", date_str):
        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
    return date_str


ensure_extra_path()


def _transcribe_with_whisper(
    audio_path: Path,
    model_name: str,
    convert_to_simplified: bool,
    logger: Optional[Callable[[str], None]],
    language_code: Optional[str] = "zh",
) -> List[Segment]:
    _require_whisper()
    _maybe_log(logger, "ğŸ§  æ­£åœ¨ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘â€¦")
    try:
        model = whisper.load_model(model_name)
    except Exception as exc:  # pragma: no cover
        raise VideoProcessingError(f"Whisper æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{exc}") from exc
    try:
        kwargs = {"language": language_code} if language_code else {}
        result = model.transcribe(str(audio_path), **kwargs)
    except Exception as exc:  # pragma: no cover
        raise VideoProcessingError(f"Whisper è¯†åˆ«å¤±è´¥ï¼š{exc}") from exc

    segments_data = result.get("segments") or []
    if not segments_data:
        text = normalize_text(result.get("text", ""), convert_to_simplified)
        if not text:
            raise VideoProcessingError("Whisper æœªè¿”å›ä»»ä½•æ–‡æœ¬")
        return [Segment(start=0.0, end=0.0, text=text)]

    segments: List[Segment] = []
    for seg in segments_data:
        raw_text = seg.get("text", "").strip()
        text = normalize_text(raw_text, convert_to_simplified)
        if not text:
            continue
        start = float(seg.get("start") or 0.0)
        end = float(seg.get("end") or start)
        segments.append(Segment(start=start, end=end, text=text))
    if not segments:
        raise VideoProcessingError("Whisper æœªè¿”å›ä»»ä½•æ–‡æœ¬")
    return segments


def _download_audio(video_url: str, output_dir: Path, base_name: str, logger: Optional[Callable[[str], None]]) -> Path:
    _require_ytdlp()
    ensure_ffmpeg_available()
    outtmpl = str(output_dir / f"{base_name}.%(ext)s")
    opts = {
        "quiet": True,
        "no_warnings": True,
        "noplaylist": True,
        "format": "bestaudio/best",
        "outtmpl": outtmpl,
    }
    _maybe_log(logger, "â¬ æ­£åœ¨ä¸‹è½½éŸ³é¢‘â€¦")
    try:
        with YoutubeDL(opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            downloaded = Path(ydl.prepare_filename(info))
            if not downloaded.exists():
                raise VideoProcessingError("éŸ³é¢‘æ–‡ä»¶æœªç”Ÿæˆï¼Œå¯èƒ½æ˜¯ä¸‹è½½è¢«ä¸­æ–­")
            return downloaded
    except VideoProcessingError:
        raise
    except Exception as exc:
        raise VideoProcessingError(f"éŸ³é¢‘ä¸‹è½½å¤±è´¥ï¼š{exc}") from exc


def _fetch_bilibili_view(bvid: str) -> Dict:
    try:
        resp = requests.get(
            BILIBILI_VIEW_API, params={"bvid": bvid}, headers={"User-Agent": USER_AGENT}, timeout=15
        )
        resp.raise_for_status()
    except requests.RequestException as exc:
        raise VideoProcessingError(f"æ— æ³•è®¿é—® B ç«™æ¥å£ï¼š{exc}") from exc
    data = resp.json().get("data")
    if not data:
        raise VideoProcessingError("æ— æ³•è·å– B ç«™è§†é¢‘ä¿¡æ¯")
    return data


def _fetch_bilibili_subtitle_entry(bvid: str, cid: str, prefer_english: bool, allow_fallback: bool = True) -> Optional[Dict]:
    try:
        resp = requests.get(
            BILIBILI_SUBTITLE_API,
            params={"bvid": bvid, "cid": cid},
            headers={"User-Agent": USER_AGENT},
            timeout=15,
        )
        resp.raise_for_status()
    except requests.RequestException as exc:
        raise VideoProcessingError(f"æ— æ³•è®¿é—® B ç«™å­—å¹•æ¥å£ï¼š{exc}") from exc
    subtitles = resp.json().get("data", {}).get("subtitle", {}).get("subtitles") or []
    if not subtitles:
        return None

    langs = ENGLISH_LANGS if prefer_english else PREFERRED_LANGS + ENGLISH_LANGS
    for lang in langs:
        for item in subtitles:
            lan = (item.get("lan") or "").lower()
            if lan == lang.lower():
                return item
    if allow_fallback and subtitles:
        return subtitles[0]
    return None


def _download_bilibili_subtitle_segments(entry: Dict, convert_to_simplified: bool) -> List[Segment]:
    url = entry.get("subtitle_url")
    if not url:
        return []
    if url.startswith("//"):
        url = "https:" + url
    try:
        resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=15)
        resp.raise_for_status()
    except requests.RequestException as exc:
        raise VideoProcessingError(f"ä¸‹è½½ B ç«™å­—å¹•å¤±è´¥ï¼š{exc}") from exc
    items = resp.json()
    segments: List[Segment] = []
    for item in items:
        text = normalize_text(item.get("content", ""), convert_to_simplified=convert_to_simplified)
        if not text:
            continue
        start = float(item.get("from") or 0.0)
        end = float(item.get("to") or start)
        segments.append(Segment(start=start, end=end, text=text))
    return segments


def _extract_bvid(value: str) -> str:
    match = re.search(r"(BV[0-9A-Za-z]+)", value, re.I)
    if not match:
        raise VideoProcessingError("æ— æ³•è§£æ BV å·ï¼Œè¯·ç¡®è®¤è¾“å…¥")
    return match.group(1)


def _extract_bilibili_mid(url: str) -> Optional[str]:
    match = re.search(r"space\.bilibili\.com/(\d+)", url)
    if match:
        return match.group(1)
    if url.isdigit():
        return url
    return None


def _resolve_bilibili_mid(url: str) -> Optional[str]:
    mid = _extract_bilibili_mid(url)
    if mid:
        return mid
    try:
        resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=15)
        resp.raise_for_status()
    except requests.RequestException:
        return None
    match = re.search(r'"mid"\s*:\s*(\d+)', resp.text)
    if match:
        return match.group(1)
    return None


def _resolve_series_mid(series_id: str) -> Optional[str]:
    url = f"https://www.bilibili.com/list/series/{series_id}"
    return _resolve_bilibili_mid(url)


def detect_bilibili_collection(url: str) -> Optional[Tuple[str, str]]:
    if not url:
        return None
    match = FAV_LIST_RE.search(url)
    if match:
        return ("fav", match.group(1))
    match = SERIES_LIST_RE.search(url)
    if match:
        return ("series", match.group(1))
    parsed = urlparse(url)
    qs = parse_qs(parsed.query or "")
    for key in FAV_QUERY_KEYS:
        if key in qs and qs[key]:
            return ("fav", qs[key][0])
    if "series_id" in qs and qs["series_id"]:
        return ("series", qs["series_id"][0])
    for candidate in ("collection_id", "sid", "season_id", "playlist_id"):
        if candidate in qs and qs[candidate]:
            return ("ugc_series", qs[candidate][0])
    return None


def fetch_bilibili_videos_via_api(mid: str, logger: Optional[Callable[[str], None]] = None) -> List[str]:
    """è°ƒç”¨ B ç«™ç©ºé—´ API è·å– UP ä¸»æ‰€æœ‰ BV å·ã€‚"""
    urls: List[str] = []
    ps = 50
    pn = 1
    while True:
        params = {
            "mid": mid,
            "ps": ps,
            "tid": 0,
            "pn": pn,
            "order": "pubdate",
            "jsonp": "json",
        }
        try:
            resp = requests.get(
                "https://api.bilibili.com/x/space/arc/search",
                params=params,
                headers={"User-Agent": USER_AGENT},
                timeout=15,
            )
            resp.raise_for_status()
        except requests.RequestException as exc:
            raise VideoProcessingError(f"æ‹‰å– B ç«™è§†é¢‘åˆ—è¡¨å¤±è´¥ï¼š{exc}") from exc

        data = resp.json().get("data") or {}
        vlist = data.get("list", {}).get("vlist") or []
        if not vlist:
            break

        for item in vlist:
            bvid = item.get("bvid")
            if not bvid:
                continue
            urls.append(f"https://www.bilibili.com/video/{bvid}")

        total = data.get("page", {}).get("count") or 0
        if len(urls) >= total:
            break
        pn += 1

        # é˜²æ­¢æ— é™å¾ªç¯
        if pn > 200:
            _maybe_log(logger, "âš ï¸ è§†é¢‘æ•°é‡è¶…è¿‡ 10000ï¼Œæå‰åœæ­¢")
            break

    return urls


def _collect_bilibili_pages(view_data: Dict) -> List[Dict]:
    pages = view_data.get("pages") or []
    if pages:
        return pages
    cid = view_data.get("cid")
    if not cid:
        return []
    return [
        {
            "cid": cid,
            "page": 1,
            "part": view_data.get("title"),
            "duration": view_data.get("duration"),
        }
    ]


def _detect_collection_from_video_input(raw_value: str, bvid: str) -> Optional[Tuple[str, str]]:
    info = detect_bilibili_collection(raw_value)
    if info:
        return info
    season_id = _fetch_ugc_season_id(bvid)
    if season_id:
        return ("ugc_series", season_id)
    return None


def _fetch_ugc_season_id(bvid: str) -> Optional[str]:
    try:
        resp = requests.get(
            "https://api.bilibili.com/x/web-interface/view/detail",
            params={"bvid": bvid},
            headers={"User-Agent": USER_AGENT},
            timeout=15,
        )
        resp.raise_for_status()
    except requests.RequestException:
        return None
    data = resp.json().get("data", {}).get("View", {})
    season = data.get("ugc_season") or {}
    season_id = season.get("id") or season.get("season_id")
    if season_id:
        return str(season_id)
    return None


def fetch_bilibili_ugc_season_videos(season_id: str, logger: Optional[Callable[[str], None]] = None) -> Tuple[List[str], str]:
    """è§£æ ugc_season æ•°æ®ï¼Œè¿”å›åˆé›†å†…æ‰€æœ‰ BVã€‚"""
    try:
        resp = requests.get(
            "https://api.bilibili.com/x/web-interface/view/detail",
            params={"season_id": season_id},
            headers={"User-Agent": USER_AGENT},
            timeout=15,
        )
        resp.raise_for_status()
    except requests.RequestException as exc:
        raise VideoProcessingError(f"æ‹‰å–åˆé›†è¯¦æƒ…å¤±è´¥ï¼š{exc}") from exc

    data = resp.json().get("data", {})
    view_data = data.get("View") or {}
    season = view_data.get("ugc_season") or {}
    sections = season.get("sections") or []
    title = season.get("title") or season.get("name") or f"åˆé›†{season_id}"
    urls: List[str] = []
    for section in sections:
        for episode in section.get("episodes") or []:
            bvid = episode.get("bvid")
            if not bvid:
                continue
            urls.append(f"https://www.bilibili.com/video/{bvid}")
    if not urls:
        raise VideoProcessingError("åˆé›†å†…æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘")
    return urls, title


def fetch_bilibili_fav_videos(media_id: str, logger: Optional[Callable[[str], None]] = None) -> Tuple[List[str], str]:
    """è·å–æ”¶è—å¤¹å…¨éƒ¨ BVã€‚"""
    urls: List[str] = []
    title = f"æ”¶è—å¤¹{media_id}"
    pn = 1
    ps = 20
    while True:
        params = {
            "media_id": media_id,
            "pn": pn,
            "ps": ps,
            "platform": "web",
            "order": "mtime",
        }
        try:
            resp = requests.get(
                "https://api.bilibili.com/x/v3/fav/resource/list",
                params=params,
                headers={"User-Agent": USER_AGENT},
                timeout=15,
            )
            resp.raise_for_status()
        except requests.RequestException as exc:
            raise VideoProcessingError(f"æ‹‰å–æ”¶è—å¤¹å¤±è´¥ï¼š{exc}") from exc

        data = resp.json().get("data") or {}
        info = data.get("info") or {}
        if info.get("title"):
            title = info["title"]
        medias = data.get("medias") or []
        if not medias:
            break
        for item in medias:
            bvid = item.get("bvid") or item.get("id")
            if not bvid:
                continue
            if not str(bvid).lower().startswith("bv"):
                continue
            urls.append(f"https://www.bilibili.com/video/{bvid}")
        if not data.get("has_more"):
            break
        pn += 1
    return urls, title


def fetch_bilibili_series_videos(series_id: str, logger: Optional[Callable[[str], None]] = None) -> Tuple[List[str], str]:
    """è·å–åˆé›†(ç³»åˆ—)å…¨éƒ¨ BVã€‚"""
    mid = _resolve_series_mid(series_id)
    if not mid:
        raise VideoProcessingError("æ— æ³•è§£æåˆé›†æ‰€å± UP ä¸»ï¼Œå¯èƒ½é“¾æ¥æ— æ•ˆ")

    urls: List[str] = []
    title = f"åˆé›†{series_id}"
    pn = 1
    ps = 100
    while True:
        params = {
            "mid": mid,
            "series_id": series_id,
            "only_normal": "true",
            "pn": pn,
            "ps": ps,
        }
        try:
            resp = requests.get(
                "https://api.bilibili.com/x/series/archives",
                params=params,
                headers={"User-Agent": USER_AGENT},
                timeout=15,
            )
            resp.raise_for_status()
        except requests.RequestException as exc:
            raise VideoProcessingError(f"æ‹‰å–åˆé›†åˆ—è¡¨å¤±è´¥ï¼š{exc}") from exc

        data = resp.json().get("data") or {}
        meta = data.get("meta") or {}
        if meta.get("name"):
            title = meta["name"]
        archives = data.get("archives") or []
        if not archives:
            break
        for archive in archives:
            bvid = archive.get("bvid")
            if not bvid:
                continue
            urls.append(f"https://www.bilibili.com/video/{bvid}")
        if len(archives) < ps:
            break
        pn += 1
        if pn > 200:
            _maybe_log(logger, "âš ï¸ åˆé›†è§†é¢‘æ•°é‡è¾ƒå¤šï¼Œå·²æå‰åœæ­¢")
            break
    return urls, title


def process_bilibili_video(
    value: str,
    model_name: str = "small",
    output_root: Path = DEFAULT_OUTPUT_ROOT,
    write_txt: bool = True,
    logger: Optional[Callable[[str], None]] = None,
    include_collection: bool = False,
    language_mode: str = LANGUAGE_AUTO,
) -> List[ProcessResult]:
    """å¤„ç† B ç«™è§†é¢‘ï¼›è‹¥å­˜åœ¨åˆ† Pï¼Œå…¨éƒ¨å¯¼å‡ºã€‚"""
    language_mode = normalize_language_mode(language_mode)
    bvid = _extract_bvid(value)
    view_data = _fetch_bilibili_view(bvid)
    uploader = view_data.get("owner", {}).get("name") or "æœªçŸ¥UPä¸»"
    upload_date = _fmt_upload_date_from_epoch(view_data.get("pubdate"))
    base_duration = format_timestamp(view_data.get("duration"))
    output_dir = ensure_output_dir("bilibili", uploader, output_root)

    # å°è¯•ä»æ ‡ç­¾æˆ–æè¿°ä¸­æ¨æ–­éŸ³é¢‘è¯­è¨€
    audio_lang_hint = None
    tags = view_data.get("tags") or []
    for tag in tags:
        tag_lower = str(tag).lower()
        if "english" in tag_lower or "è‹±è¯­" in tag_lower:
            audio_lang_hint = "en"
            break
        if "ä¸­æ–‡" in tag_lower or "chinese" in tag_lower:
            audio_lang_hint = "zh"
            break

    prefer_english = should_prefer_english(
        language_mode,
        [
            view_data.get("title"),
            view_data.get("desc"),
            view_data.get("dynamic"),
        ],
        fallback=False,
        audio_language=audio_lang_hint,
    )

    pages = _collect_bilibili_pages(view_data)
    if not pages:
        raise VideoProcessingError("æœªæ‰¾åˆ°å¯å¤„ç†çš„åˆ† P æˆ– CID")

    total_pages = len(pages)
    results: List[ProcessResult] = []
    video_title = view_data.get("title") or bvid
    base_url = f"https://www.bilibili.com/video/{bvid}"

    for idx, page in enumerate(pages, start=1):
        cid = page.get("cid")
        if not cid:
            continue
        page_number = page.get("page") or idx
        part_title = page.get("part") or f"P{page_number}"
        duration = format_timestamp(page.get("duration")) if page.get("duration") else base_duration
        full_title = video_title
        if total_pages > 1:
            full_title = f"{video_title}ï½œP{page_number:02d} {part_title}"
        page_url = f"{base_url}?p={page_number}"
        video_id = bvid if total_pages == 1 else f"{bvid}-P{page_number:02d}"

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        expected_filename = f"{video_id}_{slugify(full_title)}.md"
        expected_path = output_dir / expected_filename
        if expected_path.exists():
            _maybe_log(logger, f"â­ï¸ è·³è¿‡å·²å­˜åœ¨ï¼š{expected_filename}")
            meta = VideoMeta(
                platform="bilibili",
                video_id=video_id,
                title=full_title,
                uploader=uploader,
                upload_date=upload_date,
                source="skipped",
                url=page_url,
                duration=duration,
                processed_at=datetime.now().strftime("%Y-%m-%d"),
                original_language="Unknown",
                language="Unknown",
            )
            results.append(ProcessResult(platform="bilibili", markdown_path=expected_path, txt_path=None, meta=meta))
            continue

        segments: List[Segment] = []
        subtitle_entry = _fetch_bilibili_subtitle_entry(
            bvid,
            str(cid),
            prefer_english=prefer_english,
            allow_fallback=not prefer_english,
        )
        source = "official_subtitle"
        text_language = "English" if prefer_english else DEFAULT_LANGUAGE
        if subtitle_entry:
            subtitle_lang = subtitle_entry.get("lan") or "unknown"
            convert_flag = is_chinese_lang(subtitle_lang)
            text_language = DEFAULT_LANGUAGE if convert_flag else "English"
            _maybe_log(logger, f"ğŸ“„ æ‰¾åˆ°å®˜æ–¹å­—å¹•ï¼ˆ{subtitle_lang}ï¼‰ï¼Œè·³è¿‡éŸ³é¢‘è½¬å½•")
            segments = _download_bilibili_subtitle_segments(
                subtitle_entry,
                convert_to_simplified=convert_flag,
            )
        else:
            _maybe_log(logger, f"âš ï¸ æœªæ‰¾åˆ°å®˜æ–¹å­—å¹•ï¼Œå°†ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘")

        audio_path: Optional[Path] = None
        if not segments:
            source = "whisper"
            convert_flag = not prefer_english
            text_language = DEFAULT_LANGUAGE if convert_flag else "English"
            audio_path = _download_audio(page_url, output_dir, video_id, logger)
            try:
                whisper_lang = "en" if prefer_english else "zh"
                segments = _transcribe_with_whisper(
                    audio_path,
                    model_name,
                    convert_to_simplified=convert_flag,
                    logger=logger,
                    language_code=whisper_lang,
                )
            finally:
                if audio_path and audio_path.exists():
                    audio_path.unlink(missing_ok=True)

        if not segments:
            raise VideoProcessingError(f"åˆ† P {page_number} æœªèƒ½è·å–å­—å¹•æˆ–è¯†åˆ«æ–‡æœ¬")

        meta = VideoMeta(
            platform="bilibili",
            video_id=video_id,
            title=full_title,
            uploader=uploader,
            upload_date=upload_date,
            source=source,
            url=page_url,
            duration=duration,
            processed_at=datetime.now().strftime("%Y-%m-%d"),
            original_language="English" if prefer_english else "Chinese",
            language=text_language,
        )

        markdown_path = generate_markdown(meta, segments, output_dir)
        txt_path = None
        if write_txt:
            legacy_path = write_legacy_txt(segments, output_dir, f"{meta.video_id}_{slugify(meta.title)}")
            try:
                legacy_path.unlink()
            except FileNotFoundError:
                pass
        results.append(ProcessResult(platform="bilibili", markdown_path=markdown_path, txt_path=txt_path, meta=meta))

    if include_collection:
        collection_info = _detect_collection_from_video_input(value, bvid)
        if collection_info:
            coll_type, coll_id = collection_info
            try:
                if coll_type == "fav":
                    urls, title = fetch_bilibili_fav_videos(coll_id, logger=logger)
                elif coll_type == "series":
                    urls, title = fetch_bilibili_series_videos(coll_id, logger=logger)
                else:
                    urls, title = fetch_bilibili_ugc_season_videos(coll_id, logger=logger)
                urls = [url for url in urls if _extract_bvid(url) != bvid]
                if urls:
                    _maybe_log(logger, f"ğŸ“š æ£€æµ‹åˆ°åˆé›†ã€Š{title}ã€‹ï¼Œå…± {len(urls)} ä¸ªé¢å¤–è§†é¢‘ï¼Œè‡ªåŠ¨å¤„ç†â€¦")
                    extra_results, failures = _process_video_batch(
                        urls,
                        model_name=model_name,
                        output_root=output_root,
                        language_mode=language_mode,
                        include_collection=False,
                        logger=logger,
                        write_txt=write_txt,
                    )
                    if failures:
                        _maybe_log(logger, f"âš ï¸ åˆé›†å†…éƒ¨åˆ†è§†é¢‘å¤„ç†å¤±è´¥ï¼š{failures[:2]}")
                    results.extend(extra_results)
                else:
                    _maybe_log(logger, "âš ï¸ åˆé›†ä¸­æ²¡æœ‰é¢å¤–å¯å¤„ç†çš„è§†é¢‘")
            except VideoProcessingError as exc:
                _maybe_log(logger, f"âš ï¸ åˆé›†å¤„ç†å¤±è´¥ï¼š{exc}")
        else:
            _maybe_log(logger, "â„¹ï¸ æœªæ£€æµ‹åˆ°åˆé›†ä¿¡æ¯ï¼Œä»…å¤„ç†å½“å‰è§†é¢‘ã€‚")

    return results


def _normalize_youtube_url(value: str) -> str:
    lowered = value.strip()
    if lowered.startswith("http"):
        return lowered
    return f"https://www.youtube.com/watch?v={lowered}"


def _download_youtube_subtitles(
    video_url: str,
    temp_dir: Path,
    prefer_english: bool,
    allow_fallback: bool,
    logger: Optional[Callable[[str], None]],
) -> Tuple[List[Segment], Optional[str]]:
    _require_ytdlp()
    # æ ¹æ® prefer_english è°ƒæ•´è¯­è¨€ä¼˜å…ˆçº§ï¼ˆä¼˜åŒ–ï¼šä¼˜å…ˆä¸‹è½½åŒ¹é…è¯­è¨€çš„å­—å¹•ï¼‰
    if prefer_english:
        requested_langs = list(dict.fromkeys(ENGLISH_LANGS + PREFERRED_LANGS))
    else:
        requested_langs = list(dict.fromkeys(PREFERRED_LANGS + ENGLISH_LANGS))
    opts = {
        "quiet": True,
        "no_warnings": True,
        "skip_download": True,
        "writesubtitles": True,
        "writeautomaticsub": True,
        "subtitlesformat": "vtt",
        "subtitleslangs": requested_langs,
        "outtmpl": str(temp_dir / "%(id)s"),
    }
    _maybe_log(logger, "å°è¯•ä¸‹è½½ YouTube å­—å¹•â€¦")
    try:
        with YoutubeDL(opts) as ydl:
            ydl.download([video_url])
    except Exception as e:
        # å…è®¸éƒ¨åˆ†ä¸‹è½½å¤±è´¥ï¼ˆä¾‹å¦‚è¯·æ±‚ä¸å­˜åœ¨çš„è¯­è¨€ï¼‰ï¼Œåªè¦æœ‰ä»»ä½•å­—å¹•ä¸‹è½½æˆåŠŸå³å¯
        _maybe_log(logger, f"å­—å¹•ä¸‹è½½æ—¶å‡ºç°é”™è¯¯ï¼ˆå°†å°è¯•ä½¿ç”¨å·²ä¸‹è½½çš„å­—å¹•ï¼‰: {str(e)[:100]}")

    vtt_files = list(temp_dir.glob("*.vtt"))
    lang_map: Dict[str, Path] = {}
    for file in vtt_files:
        parts = file.name.split(".")
        lang = parts[-2] if len(parts) >= 3 else "unknown"
        lang_map[lang] = file

    lang_priority = ENGLISH_LANGS if prefer_english else requested_langs
    fallback_langs: List[str] = []
    if not prefer_english or allow_fallback:
        fallback_langs = [lang for lang in requested_langs if lang not in lang_priority]

    checked_langs = list(dict.fromkeys(lang_priority + fallback_langs))
    for lang in checked_langs:
        file = None
        for key, value in lang_map.items():
            if key.lower() == lang.lower():
                file = value
                break
        if not file:
            continue
        try:
            text = file.read_text(encoding="utf-8", errors="ignore")
        except OSError:
            continue
        segments = parse_subtitle_text(text)
        if not segments:
            continue
        convert_flag = is_chinese_lang(lang)
        normalized_segments = [
            Segment(seg.start, seg.end, normalize_text(seg.text, convert_flag)) for seg in segments
        ]
        return normalized_segments, lang

    return [], None


def _extract_youtube_info(video_url: str) -> Dict:
    _require_ytdlp()
    opts = {"quiet": True, "no_warnings": True, "skip_download": True, "noplaylist": True}
    try:
        with YoutubeDL(opts) as ydl:
            info = ydl.extract_info(video_url, download=False)
    except Exception as exc:
        raise VideoProcessingError(f"æ— æ³•è·å– YouTube è§†é¢‘ä¿¡æ¯ï¼š{exc}") from exc
    if not info:
        raise VideoProcessingError("æ— æ³•è·å– YouTube è§†é¢‘ä¿¡æ¯")
    return info


def process_youtube_video(
    value: str,
    model_name: str = "small",
    output_root: Path = DEFAULT_OUTPUT_ROOT,
    write_txt: bool = True,
    logger: Optional[Callable[[str], None]] = None,
    language_mode: str = LANGUAGE_AUTO,
) -> List[ProcessResult]:
    """å¤„ç† YouTube å•è§†é¢‘ã€‚"""
    language_mode = normalize_language_mode(language_mode)
    video_url = _normalize_youtube_url(value)
    info = _extract_youtube_info(video_url)
    video_id = info.get("id") or re.search(r"v=([\w-]+)", video_url)
    if isinstance(video_id, re.Match):
        video_id = video_id.group(1)
    if not video_id:
        raise VideoProcessingError("æ— æ³•è§£æ YouTube è§†é¢‘ ID")

    uploader = info.get("uploader") or info.get("channel") or "Unknown Channel"
    upload_date = _fmt_upload_date_from_str(info.get("upload_date"))
    duration = format_timestamp(info.get("duration"))
    original_language = info.get("language") or info.get("original_language") or "unknown"
    output_dir = ensure_output_dir("youtube", uploader, output_root)

    video_title = info.get("title") or video_id
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    expected_filename = f"{video_id}_{slugify(video_title)}.md"
    expected_path = output_dir / expected_filename
    if expected_path.exists():
        _maybe_log(logger, f"â­ï¸ è·³è¿‡å·²å­˜åœ¨ï¼š{expected_filename}")
        meta = VideoMeta(
            platform="youtube",
            video_id=video_id,
            title=video_title,
            uploader=uploader,
            upload_date=_fmt_upload_date_from_str(info.get("upload_date")),
            source="skipped",
            url=info.get("webpage_url") or video_url,
            duration=duration,
            processed_at=datetime.now().strftime("%Y-%m-%d"),
            original_language=original_language,
            language="Unknown",
        )
        return [ProcessResult(platform="youtube", markdown_path=expected_path, txt_path=None, meta=meta)]

    # YouTube æä¾›æ›´å‡†ç¡®çš„è¯­è¨€ä¿¡æ¯
    audio_lang = info.get("audio_language") or info.get("language") or original_language

    prefer_english = should_prefer_english(
        language_mode,
        [
            info.get("title"),
            info.get("description"),
        ],
        fallback=is_english_language(original_language),
        audio_language=audio_lang,
    )

    segments: List[Segment] = []
    subtitle_lang: Optional[str] = None
    convert_flag = True
    text_language = DEFAULT_LANGUAGE
    with tempfile.TemporaryDirectory() as tmp_dir:
        segments, subtitle_lang = _download_youtube_subtitles(
            video_url,
            Path(tmp_dir),
            prefer_english=prefer_english,
            allow_fallback=not prefer_english,
            logger=logger,
        )
    if segments:
        convert_flag = is_chinese_lang(subtitle_lang)
        text_language = DEFAULT_LANGUAGE if convert_flag else "English"
        _maybe_log(logger, f"ğŸ“„ æ‰¾åˆ°å®˜æ–¹å­—å¹•ï¼ˆ{subtitle_lang}ï¼‰ï¼Œè·³è¿‡éŸ³é¢‘è½¬å½•")
    else:
        _maybe_log(logger, f"âš ï¸ æœªæ‰¾åˆ°å®˜æ–¹å­—å¹•ï¼Œå°†ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘")
    source = "official_subtitle" if segments else "whisper"

    audio_path: Optional[Path] = None
    if not segments:
        convert_flag = not prefer_english
        audio_path = _download_audio(video_url, output_dir, video_id, logger)
        try:
            whisper_lang = "en" if prefer_english else "zh"
            segments = _transcribe_with_whisper(
                audio_path,
                model_name,
                convert_to_simplified=convert_flag,
                logger=logger,
                language_code=whisper_lang,
            )
            text_language = DEFAULT_LANGUAGE if convert_flag else "English"
        finally:
            if audio_path and audio_path.exists():
                audio_path.unlink(missing_ok=True)

    if not segments:
        raise VideoProcessingError("æœªèƒ½è·å–åˆ°ä»»ä½•å­—å¹•æˆ–è¯†åˆ«æ–‡æœ¬")

    meta = VideoMeta(
        platform="youtube",
        video_id=video_id,
        title=info.get("title") or video_id,
        uploader=uploader,
        upload_date=upload_date,
        source=source,
        url=info.get("webpage_url") or video_url,
        duration=duration,
        processed_at=datetime.now().strftime("%Y-%m-%d"),
        original_language="English" if prefer_english else original_language,
        language=text_language,
        tags=info.get("tags") or [],
    )

    markdown_path = generate_markdown(meta, segments, output_dir)
    txt_path = None
    if write_txt:
        legacy_path = write_legacy_txt(segments, output_dir, f"{meta.video_id}_{slugify(meta.title)}")
        try:
            legacy_path.unlink()
        except FileNotFoundError:
            pass
    return [ProcessResult(platform="youtube", markdown_path=markdown_path, txt_path=txt_path, meta=meta)]


def _fetch_creator_video_urls_ytdlp(
    channel_url: str,
    platform: str,
    cookie_file: Optional[Path] = None,
) -> List[str]:
    """ä½¿ç”¨ yt-dlp è·å–åšä¸»å…¨éƒ¨è§†é¢‘ URLã€‚"""
    _require_ytdlp()
    opts = {
        "quiet": True,
        "no_warnings": True,
        "skip_download": True,
        "extract_flat": True,
        "force_generic_extractor": False,
    }
    if cookie_file and cookie_file.exists():
        opts["cookiefile"] = str(cookie_file)
    try:
        with YoutubeDL(opts) as ydl:
            info = ydl.extract_info(channel_url, download=False)
    except Exception as exc:
        text = str(exc)
        if "Rejected by server (352)" in text:
            hint = (
                "B ç«™é™åˆ¶è®¿é—®ï¼Œè¯·å…ˆåœ¨æµè§ˆå™¨ç™»å½•åå¯¼å‡º Cookie åˆ° bilibili_cookie.txtï¼Œ"
                "å¹¶è®¾ç½®ç¯å¢ƒå˜é‡ BILIBILI_COOKIE_FILE æŒ‡å‘è¯¥æ–‡ä»¶ã€‚"
            )
            raise VideoProcessingError(f"{text}ã€‚{hint}") from exc
        raise VideoProcessingError(f"æ— æ³•è·å–åˆ›ä½œè€…è§†é¢‘åˆ—è¡¨ï¼š{exc}") from exc

    if not info:
        return []

    urls: List[str] = []

    def _walk_entries(entry: Dict) -> Iterable[Dict]:
        entries = entry.get("entries") or []
        for item in entries:
            if not item:
                continue
            if item.get("_type") == "playlist":
                yield from _walk_entries(item)
            else:
                yield item

    for entry in _walk_entries(info) if info.get("entries") else []:
        video_url = entry.get("webpage_url") or entry.get("url") or entry.get("id")
        if not video_url:
            continue
        if not video_url.startswith("http"):
            if platform == "youtube":
                video_url = f"https://www.youtube.com/watch?v={video_url}"
            else:
                video_url = f"https://www.bilibili.com/video/{video_url}"
        urls.append(video_url)

    # fallbackï¼šå½“ info æœ¬èº«æ˜¯ flat åˆ—è¡¨
    if not urls and info.get("webpage_url"):
        entry_url = info.get("webpage_url")
        if entry_url:
            urls.append(entry_url)

    # å»é‡ä¿æŒé¡ºåº
    seen = set()
    unique_urls: List[str] = []
    for url in urls:
        if url in seen:
            continue
        seen.add(url)
        unique_urls.append(url)
    return unique_urls


class _ThreadSafeLogger:
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—åŒ…è£…å™¨"""

    def __init__(self, logger: Optional[Callable[[str], None]]):
        self.logger = logger
        self.lock = Lock()

    def log(self, message: str) -> None:
        if self.logger:
            with self.lock:
                self.logger(message)


def _process_single_video(
    video_url: str,
    model_name: str,
    output_root: Path,
    language_mode: str,
    include_collection: bool,
    write_txt: bool,
    logger: Optional[Callable[[str], None]],
) -> Tuple[Optional[List[ProcessResult]], Optional[str]]:
    """å¤„ç†å•ä¸ªè§†é¢‘ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
    try:
        platform = detect_platform(video_url) or ("youtube" if "youtu" in video_url else "bilibili")
        if platform == "bilibili":
            processed = process_bilibili_video(
                video_url,
                model_name=model_name,
                output_root=output_root,
                logger=logger,
                include_collection=include_collection,
                language_mode=language_mode,
                write_txt=write_txt,
            )
        elif platform == "youtube":
            processed = process_youtube_video(
                video_url,
                model_name=model_name,
                output_root=output_root,
                logger=logger,
                language_mode=language_mode,
                write_txt=write_txt,
            )
        else:
            raise VideoProcessingError("æ— æ³•è¯†åˆ«è§†é¢‘å¹³å°")

        if not isinstance(processed, list):
            processed = [processed]
        return processed, None
    except VideoProcessingError as exc:
        return None, f"{video_url} -> {exc}"
    except Exception as exc:  # pragma: no cover
        return None, f"{video_url} -> æœªçŸ¥é”™è¯¯ï¼š{exc}"


def _process_video_batch(
    video_urls: List[str],
    model_name: str,
    output_root: Path,
    language_mode: str = LANGUAGE_AUTO,
    include_collection: bool = False,
    write_txt: bool = True,
    logger: Optional[Callable[[str], None]] = None,
    max_workers: int = DEFAULT_MAX_WORKERS,
    enable_concurrent: bool = ENABLE_CONCURRENT,
) -> Tuple[List[ProcessResult], List[str]]:
    """æ‰¹é‡å¤„ç†è§†é¢‘ï¼Œæ”¯æŒå¹¶å‘å¤„ç†"""
    if not video_urls:
        raise VideoProcessingError("æœªè·å–åˆ°ä»»ä½•è§†é¢‘ï¼Œå¯èƒ½é“¾æ¥æ— æ•ˆæˆ–ä¸å¯è®¿é—®")

    successful: List[ProcessResult] = []
    failures: List[str] = []
    total = len(video_urls)

    # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
    max_workers = min(max(1, max_workers), MAX_WORKERS_LIMIT)

    # å¦‚æœåªæœ‰1-2ä¸ªè§†é¢‘æˆ–ç¦ç”¨å¹¶å‘ï¼Œä½¿ç”¨é¡ºåºå¤„ç†
    if total <= 2 or not enable_concurrent:
        _maybe_log(logger, f"å…±è·å– {total} ä¸ªè§†é¢‘ï¼Œå¼€å§‹é¡ºåºå¤„ç†â€¦")
        for idx, video_url in enumerate(video_urls, start=1):
            _maybe_log(logger, f"==> æ­£åœ¨å¤„ç† {idx}/{total}ï¼š{video_url}")
            processed, error = _process_single_video(
                video_url, model_name, output_root, language_mode, include_collection, write_txt, logger
            )
            if processed:
                successful.extend(processed)
                last_path = processed[-1].markdown_path.name if processed else "unknown"
                _maybe_log(logger, f"   âœ… å®Œæˆï¼š{last_path}")
            else:
                failures.append(error or f"{video_url} -> æœªçŸ¥é”™è¯¯")
                _maybe_log(logger, f"   âš ï¸ è·³è¿‡ï¼š{error}")
    else:
        # å¹¶å‘å¤„ç†
        _maybe_log(logger, f"å…±è·å– {total} ä¸ªè§†é¢‘ï¼Œå¼€å§‹å¹¶å‘å¤„ç†ï¼ˆ{max_workers} çº¿ç¨‹ï¼‰â€¦")
        safe_logger = _ThreadSafeLogger(logger)
        completed_count = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_url = {
                executor.submit(
                    _process_single_video,
                    url,
                    model_name,
                    output_root,
                    language_mode,
                    include_collection,
                    write_txt,
                    safe_logger.log,
                ): url
                for url in video_urls
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_url):
                video_url = future_to_url[future]
                completed_count += 1

                try:
                    processed, error = future.result()
                    if processed:
                        successful.extend(processed)
                        last_path = processed[-1].markdown_path.name if processed else "unknown"
                        safe_logger.log(f"âœ… [{completed_count}/{total}] å®Œæˆï¼š{last_path}")
                    else:
                        failures.append(error or f"{video_url} -> æœªçŸ¥é”™è¯¯")
                        safe_logger.log(f"âš ï¸ [{completed_count}/{total}] è·³è¿‡ï¼š{error}")
                except Exception as exc:  # pragma: no cover
                    error_msg = f"{video_url} -> æ‰§è¡Œå¼‚å¸¸ï¼š{exc}"
                    failures.append(error_msg)
                    safe_logger.log(f"âš ï¸ [{completed_count}/{total}] å¼‚å¸¸ï¼š{exc}")

    _maybe_log(
        logger,
        f"æ‰¹é‡å®Œæˆï¼šæˆåŠŸ {len(successful)} / {total}ï¼Œå¤±è´¥ {len(failures)}ã€‚",
    )
    return successful, failures


def export_creator_videos(
    creator_url: str,
    model_name: str = "small",
    output_root: Path = DEFAULT_OUTPUT_ROOT,
    limit: int = 0,
    language_mode: str = LANGUAGE_AUTO,
    logger: Optional[Callable[[str], None]] = None,
    max_workers: int = DEFAULT_MAX_WORKERS,
    enable_concurrent: bool = ENABLE_CONCURRENT,
) -> Tuple[List[ProcessResult], List[str]]:
    """æ‰¹é‡å¯¼å‡ºåˆ›ä½œè€…å…¨éƒ¨è§†é¢‘ï¼Œæ”¯æŒå¹¶å‘å¤„ç†ï¼Œè¿”å›æˆåŠŸç»“æœä¸å¤±è´¥åŸå› ã€‚"""
    platform = detect_platform(creator_url or "")
    if platform not in {"bilibili", "youtube"}:
        raise VideoProcessingError("æ— æ³•è¯†åˆ«å¹³å°ï¼Œç›®å‰ä»…æ”¯æŒ B ç«™å’Œ YouTube")

    bilibili_cookie = Path(os.getenv("BILIBILI_COOKIE_FILE", "bilibili_cookie.txt")).expanduser()
    youtube_cookie = Path(os.getenv("YOUTUBE_COOKIE_FILE", "youtube_cookie.txt")).expanduser()
    cookie_file = bilibili_cookie if platform == "bilibili" else youtube_cookie
    if not cookie_file.exists():
        cookie_file = None

    resolved_mid = _resolve_bilibili_mid(creator_url) if platform == "bilibili" else None
    video_urls: List[str] = []
    if platform == "bilibili" and resolved_mid:
        try:
            video_urls = fetch_bilibili_videos_via_api(resolved_mid, logger=logger)
        except VideoProcessingError as exc:
            _maybe_log(logger, f"âš ï¸ å®˜æ–¹ API è·å–å¤±è´¥ï¼š{exc}ï¼Œå°è¯• yt-dlpâ€¦")

    if not video_urls:
        video_urls = _fetch_creator_video_urls_ytdlp(creator_url, platform, cookie_file=cookie_file)
    if not video_urls:
        raise VideoProcessingError("æœªè·å–åˆ°ä»»ä½•è§†é¢‘ï¼Œå¯èƒ½æ˜¯ä¸»é¡µä¸å¯è®¿é—®æˆ–æ— å…¬å¼€è§†é¢‘")

    if limit and limit > 0:
        video_urls = video_urls[:limit]

    return _process_video_batch(
        video_urls,
        model_name,
        output_root,
        language_mode=language_mode,
        include_collection=False,
        logger=logger,
        max_workers=max_workers,
        enable_concurrent=enable_concurrent,
    )


def export_bilibili_collection_videos(
    collection_url: str,
    model_name: str = "small",
    output_root: Path = DEFAULT_OUTPUT_ROOT,
    limit: int = 0,
    language_mode: str = LANGUAGE_AUTO,
    logger: Optional[Callable[[str], None]] = None,
    max_workers: int = DEFAULT_MAX_WORKERS,
    enable_concurrent: bool = ENABLE_CONCURRENT,
) -> Tuple[List[ProcessResult], List[str]]:
    info = detect_bilibili_collection(collection_url)
    if not info:
        # å…è®¸ç”¨æˆ·ç›´æ¥ç²˜è´´åˆé›†ä¸­çš„å•ä¸ª BV é“¾æ¥
        try:
            bvid = _extract_bvid(collection_url)
        except VideoProcessingError as exc:
            raise VideoProcessingError("æ— æ³•è¯†åˆ« B ç«™åˆé›†/æ”¶è—å¤¹é“¾æ¥") from exc
        inferred = _detect_collection_from_video_input(collection_url, bvid)
        if not inferred:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šPè§†é¢‘ï¼ˆä¸æ˜¯åˆé›†ï¼Œè€Œæ˜¯å•ä¸ªè§†é¢‘çš„å¤šä¸ªåˆ†Pï¼‰
            try:
                view_data = _fetch_bilibili_view(bvid)
                pages_count = len(view_data.get("pages", []))
                if pages_count > 1:
                    _maybe_log(logger, f"âš ï¸ æ£€æµ‹åˆ°è¿™æ˜¯ä¸€ä¸ªå¤šåˆ†Pè§†é¢‘ï¼ˆå…±{pages_count}ä¸ªåˆ†Pï¼‰ï¼Œéåˆé›†ã€‚è‡ªåŠ¨åˆ‡æ¢åˆ°å•è§†é¢‘å¤„ç†æ¨¡å¼...")
                    results = process_bilibili_video(
                        collection_url,
                        model_name=model_name,
                        output_root=output_root,
                        logger=logger,
                        include_collection=False,
                        language_mode=language_mode,
                        write_txt=True,
                    )
                    return results, []
            except Exception:
                pass
            raise VideoProcessingError("æ— æ³•è¯†åˆ« B ç«™åˆé›†/æ”¶è—å¤¹é“¾æ¥ï¼Œè¯·ç¡®è®¤é“¾æ¥æ˜¯å¦æ­£ç¡®")
        info = inferred
    coll_type, coll_id = info

    if coll_type == "fav":
        video_urls, title = fetch_bilibili_fav_videos(coll_id, logger=logger)
    elif coll_type == "series":
        video_urls, title = fetch_bilibili_series_videos(coll_id, logger=logger)
    else:
        video_urls, title = fetch_bilibili_ugc_season_videos(coll_id, logger=logger)

    if not video_urls:
        raise VideoProcessingError("åˆé›†å†…æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œæˆ–è®¿é—®å—é™")

    if limit and limit > 0:
        video_urls = video_urls[:limit]

    _maybe_log(logger, f"åˆé›†ã€Š{title}ã€‹å…± {len(video_urls)} ä¸ªè§†é¢‘ã€‚")
    return _process_video_batch(
        video_urls,
        model_name,
        output_root,
        language_mode=language_mode,
        include_collection=False,
        logger=logger,
        max_workers=max_workers,
        enable_concurrent=enable_concurrent,
    )
